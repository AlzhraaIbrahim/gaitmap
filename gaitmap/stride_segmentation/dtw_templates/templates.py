"""Dtw template base classes and helper."""
from functools import reduce
from importlib.resources import open_text
from typing import List, Optional, Tuple, Union, cast, Sequence, Iterable

import numpy as np
import pandas as pd
from tpcp import OptimizableParameter, OptimizableAlgorithm, cf
from typing_extensions import Self

from gaitmap.base import BaseAlgorithm
from gaitmap.data_transform import BaseTransformer, FixedScaler, TrainableTransformerMixin
from gaitmap.utils._types import _Hashable
from gaitmap.utils.array_handling import multi_array_interpolation
from gaitmap.utils.datatype_helper import is_single_sensor_data, SingleSensorData


class BaseDtwTemplate(BaseAlgorithm):
    use_cols: Optional[Sequence[Union[str, int]]]

    def __init__(
        self, *, scaling: Optional[BaseTransformer] = None, use_cols: Optional[Sequence[Union[str, int]]] = None,
    ):
        self.scaling = scaling
        self.use_cols = use_cols

    def get_data(self) -> Union[np.ndarray, pd.DataFrame]:
        raise NotImplementedError

    def _apply_scaling(self, data, sampling_rate_hz: float) -> SingleSensorData:
        if not self.scaling:
            return data
        if isinstance(data, np.ndarray):
            raise ValueError(
                "Data Transformations are only supported for dataframe templates at the moment."
                "Explicitly set `self.scaling` to None."
            )
        return self.scaling.clone().transform(data, sampling_rate_hz=sampling_rate_hz).transformed_data_

    def transform_data(self, data: SingleSensorData, sampling_rate_hz: float) -> SingleSensorData:
        return self._apply_scaling(data, sampling_rate_hz)


class BarthOriginalTemplate(BaseDtwTemplate):
    """Template used for stride segmentation by Barth et al.

    Parameters
    ----------
    scaling
        A multiplicative factor used to downscale the signal before the template is applied.
        The downscaled signal should then have have the same value range as the template signal.
        A large scale difference between data and template will result in mismatches.
        At the moment only homogeneous scaling of all axis is supported.
        Note that the actual use of the scaling depends on the DTW implementation and not all DTW class might use the
        scaling factor in the same way.
        For this template the default value is 500, which is adapted for data that has a max-gyro peak of approx.
        500 deg/s in `gyr_ml` during the swing phase.
        This is appropriate for most walking styles.
    use_cols
        The columns of the template that should actually be used.
        The default (all gyro axis) should work well, but will not match turning stride.
        Note, that this template only consists of gyro data (i.e. you can only select one of
        :obj:`~gaitmap.utils.consts.BF_GYR`)

    Notes
    -----
    As this template was generated by interpolating multiple strides, it does not really have a single sampling rate.
    The original were all recorded at 102.4 Hz, but as the template is interpolated to 200 samples, its is closer to an
    effective sampling rate of 200 Hz (a normal stride is around 0.8-1.5s).
    This template reports a sampling rate of 204.8 Hz, to prevent resampling for this very common sampling rate.

    See Also
    --------
    gaitmap.stride_segmentation.DtwTemplate: Base class for templates
    gaitmap.stride_segmentation.BarthDtw: How to apply templates for stride segmentation

    """

    template_file_name = "barth_original_template.csv"
    sampling_rate_hz = 204.8

    def __init__(self, *, scaling=cf(FixedScaler(scale=500.0)), use_cols: Optional[Sequence[Union[str, int]]] = None):
        super().__init__(scaling=scaling, use_cols=use_cols)

    def get_data(self) -> Union[np.ndarray, pd.DataFrame]:
        """Return the template of the dataset."""
        with open_text("gaitmap.stride_segmentation.dtw_templates", cast(str, self.template_file_name)) as test_data:
            data = pd.read_csv(test_data, header=0)
        template = data

        if self.use_cols is None:
            use_cols = template.columns
        else:
            use_cols = self.use_cols
        return self._apply_scaling(template[list(use_cols)], self.sampling_rate_hz)


class DtwTemplate(BaseDtwTemplate, OptimizableAlgorithm):
    """Wrap all required information about a dtw template.

    Parameters
    ----------
    data
        The actual data representing the template.
        If this should be a array or a dataframe might depend on your usecase.
        Usually it is a good idea to scale the data to a range from 0-1 and then use the scale parameter to downscale
        the signal.
        Do not use this attribute directly to access the data, but use the `get_data` method instead.
    sampling_rate_hz
        The sampling rate that was used to record the template data
    scaling
        A multiplicative factor used to downscale the signal before the template is applied.
        The downscaled signal should then have the same value range as the template signal.
        A large scale difference between data and template will result in mismatches.
        At the moment only homogeneous scaling of all axis is supported.
        Note that the actual use of the scaling depends on the DTW implementation and not all DTW class might use the
        scaling factor in the same way.
    use_cols
        The columns of the template that should actually be used.
        If the template is an array this must be a list of **int**, if it is a dataframe, the content of `use_cols`
        must match a subset of these columns.

    See Also
    --------
    gaitmap.stride_segmentation.BaseDtw: How to apply templates
    gaitmap.stride_segmentation.BarthDtw: How to apply templates for stride segmentation

    """

    sampling_rate_hz: Optional[float]
    template_file_name: Optional[str]
    scaling: Optional[BaseTransformer]
    data: Optional[Union[np.ndarray, pd.DataFrame]]

    def __init__(
        self,
        *,
        data: Optional[Union[np.ndarray, pd.DataFrame]] = None,
        sampling_rate_hz: Optional[float] = None,
        scaling: Optional[BaseTransformer] = None,
        use_cols: Optional[Sequence[Union[str, int]]] = None,
    ):

        self.data = data
        self.sampling_rate_hz = sampling_rate_hz
        super().__init__(scaling=scaling, use_cols=use_cols)

    def get_data(self) -> Union[np.ndarray, pd.DataFrame]:
        """Return the template of the dataset."""
        data = self.data
        if data is None:
            raise ValueError(
                "No data was provided for this template. "
                "Either pass a dataframe or a numpy array to the constructor or use `self_optimize` (if "
                "implemented by the Template class you are using) to calculate a template from example "
                "data."
            )
        template = data

        if self.use_cols is None:
            return self._apply_scaling(template, self.sampling_rate_hz)
        use_cols = list(self.use_cols)
        if isinstance(template, np.ndarray):
            if template.ndim < 2:
                raise ValueError("The stored template is only 1D, but a 2D array is required to use `use_cols`")
            return np.squeeze(template[:, use_cols])
        return self._apply_scaling(template[use_cols], self.sampling_rate_hz)

    def self_optimize(
        self,
        data_sequences: Iterable[SingleSensorData],
        sampling_rate_hz: Optional[float] = None,
        *,
        columns: Optional[List[_Hashable]] = None,
        **_,
    ) -> Self:
        """Create a new template by averaging the data of the given sequences.

        All sequences must have the same length.
        If this is not the case, you should use `gaitmap.stride_segmentation.InterpolatedDtwTemplate` instead.

        Parameters
        ----------
        data_sequences
            The sequences of data that should be used to create the template.
        sampling_rate_hz
            The sampling rate that was used to record the data sequences.
        columns
            The columns of the data that should actually be used for the template.
            If None, all columns will be used.
        """
        template_df = _create_average_template(data_sequences, columns=columns)
        self.sampling_rate_hz = sampling_rate_hz
        if isinstance(self.scaling, TrainableTransformerMixin):
            self.scaling = self.scaling.self_optimize([template_df], sampling_rate_hz=self.sampling_rate_hz)
        self.data = template_df
        return self


class InterpolatedDtwTemplate(DtwTemplate):
    scaling: OptimizableParameter[Optional[BaseTransformer]]
    data: OptimizableParameter[Optional[Union[np.ndarray, pd.DataFrame]]]
    sampling_rate_hz: OptimizableParameter[Optional[float]]

    def __init__(
        self,
        *,
        data: Optional[Union[np.ndarray, pd.DataFrame]] = None,
        sampling_rate_hz: Optional[float] = None,
        scaling: Optional[BaseTransformer] = None,
        interpolation_method: str = "linear",
        n_samples: Optional[int] = None,
        use_cols: Optional[Sequence[Union[str, int]]] = None,
    ):
        self.interpolation_method = interpolation_method
        self.n_samples = n_samples
        super().__init__(
            data=data, sampling_rate_hz=sampling_rate_hz, scaling=scaling, use_cols=use_cols,
        )

    def self_optimize(
        self,
        data_sequences: Iterable[SingleSensorData],
        sampling_rate_hz: Optional[float] = None,
        *,
        columns: Optional[List[_Hashable]] = None,
        **_,
    ):
        template_df, effective_sampling_rate = _create_interpolated_dtw_template(
            data_sequences, sampling_rate_hz, kind=self.interpolation_method, n_samples=self.n_samples, columns=columns,
        )
        self.sampling_rate_hz = effective_sampling_rate
        if isinstance(self.scaling, TrainableTransformerMixin):
            self.scaling = self.scaling.self_optimize([template_df], sampling_rate_hz=self.sampling_rate_hz)
        self.data = template_df
        return self


def _create_interpolated_dtw_template(
    signal_sequences: Iterable[SingleSensorData],
    sampling_rate_hz: float,
    kind: str = "linear",
    n_samples: Optional[int] = None,
    columns: Optional[List[_Hashable]] = None,
) -> Tuple[pd.DataFrame, float]:
    expected_col_order = columns
    arrays = []
    for df in signal_sequences:
        if expected_col_order is None:
            expected_col_order = df.columns
        is_single_sensor_data(df, check_acc=False, check_gyr=False, frame="any", raise_exception=True)
        arrays.append(df.to_numpy())

    del signal_sequences
    # get mean stride length over given strides
    mean_stride_samples = int(np.rint(np.mean([len(df) for df in arrays])))
    n_samples = n_samples or mean_stride_samples
    resampled_sequences_df_list = multi_array_interpolation(arrays, n_samples, kind=kind)

    template = np.mean(resampled_sequences_df_list, axis=0)
    template_df = pd.DataFrame(template.T, columns=expected_col_order)

    # When we interpolate all templates to a fixed number of samples, the effective sampling rate changes.
    # We approximate the sampling rate using the average stride length in the provided data.
    effective_sampling_rate = sampling_rate_hz
    if n_samples and sampling_rate_hz:
        effective_sampling_rate = n_samples / (mean_stride_samples / sampling_rate_hz)

    return template_df, effective_sampling_rate


def _create_average_template(
    signal_sequences: Iterable[SingleSensorData], columns: Optional[List[_Hashable]] = None,
):
    expected_col_order = columns
    n_sequences = 1

    # We do all this fancy logic to avoid creating a list of objects and keep them as generators and iterate only once.
    def process_single_frame(df_old: pd.DataFrame, df_new: pd.DataFrame) -> pd.DataFrame:
        nonlocal expected_col_order
        nonlocal n_sequences
        n_sequences += 1
        if expected_col_order is None:
            expected_col_order = df_new.columns
        return df_old.add(df_new, fill_value=0)

    df = reduce(process_single_frame, signal_sequences)
    df /= n_sequences

    return df
